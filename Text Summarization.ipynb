{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edc1959",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "With the dawn of the internet, utilizing information has become pervasive but the rapid growth of information causes the problem of information overload. In this large amount of information, how to find the right information which meets customer needs. In this context, Recommender System can help us to deal with such huge information. Also, with the increase in user options and rapid change in user preferences, we need some online systems that quickly adapt and recommend the relevant items.\n",
    "\n",
    "A recommender system computes and suggests the relevant items based on user details, content details, and their interaction logs such as ratings. For example, Netflix is a streaming platform that recommends movies and series and keeps the consumer engaged on their platform. This engagement motivates customers to renew their subscriptions.\n",
    "\n",
    "Content-based recommender system uses descriptive details products in order to make recommendations. For example, if the user has liked a web series in the past and the feature of that web series comedy genre then Recommender System will recommend the next series or movie based on the same genre. So the system adapts and learns the user behavior and suggests the items based on that behavior. In this article, we are using movie description or overview text to discover similar movies for recommendation using text similarity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Abstractive : own words  ==> Seq2Seq models \n",
    "    \n",
    "Extractive : select few sentences from the article that represent the summary of it.\n",
    "    ==> Page Rank ==> Text Rank, Lex Rank, LSA, and KL Divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70e9f9",
   "metadata": {},
   "source": [
    "## Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3775fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 148 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /home/avinash/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/avinash/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/avinash/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0e5af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28406/3350065596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Summarize text using gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgen_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "# Summarize text using gensim\n",
    "gen_summary=summarize(text)\n",
    "\n",
    "# print summary\n",
    "print(gen_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d691a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/sumy/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/sumy/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/sumy/\u001b[0m\n",
      "Collecting sumy\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/19/46/77859104e7c3e12dfa2e5c0e27b5dd1e14cb2409b50f9c936a48f29ceaee/sumy-0.11.0-py2.py3-none-any.whl\u001b[0m\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 11 kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /home/avinash/anaconda3/lib/python3.9/site-packages (from sumy) (2.26.0)\n",
      "Collecting docopt<0.7,>=0.6.1\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting breadability>=0.1.20\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "Collecting pycountry>=18.2.23\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 410 kB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /home/avinash/anaconda3/lib/python3.9/site-packages (from sumy) (3.6.5)\n",
      "Requirement already satisfied: chardet in /home/avinash/anaconda3/lib/python3.9/site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in /home/avinash/anaconda3/lib/python3.9/site-packages (from breadability>=0.1.20->sumy) (4.6.3)\n",
      "Requirement already satisfied: click in /home/avinash/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/avinash/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/avinash/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /home/avinash/anaconda3/lib/python3.9/site-packages (from nltk>=3.0.2->sumy) (4.62.3)\n",
      "Requirement already satisfied: setuptools in /home/avinash/anaconda3/lib/python3.9/site-packages (from pycountry>=18.2.23->sumy) (58.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/avinash/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/avinash/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/avinash/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/avinash/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->sumy) (3.2)\n",
      "Building wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21711 sha256=47ae4547388eb47ab902de99703a85584b0c0fce5b68f6a1431648eb640ad5aa\n",
      "  Stored in directory: /home/avinash/.cache/pip/wheels/ba/9f/70/7795228568b81b57a8932755938da9fb1f291b0576752604aa\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=8ced76587d209969b4b2256b6d64343833e44b808758b9bc5eb0c5b4e5a91057\n",
      "  Stored in directory: /home/avinash/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "  Building wheel for pycountry (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=b835f57995c5e7e9f7356c943b68569107d36ad54c2f5b5f977404b19a5c082d\n",
      "  Stored in directory: /home/avinash/.cache/pip/wheels/47/15/92/e6dc85fcb0686c82e1edbcfdf80cfe4808c058813fed0baa8f\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: docopt, pycountry, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66d338d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the dawn of the internet, utilizing information has become pervasive but the rapid growth of information causes the problem of information overload.For example, if the user has liked a web series in the past and the feature of that web series comedy genre then Recommender System will recommend the next series or movie based on the same genre.\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "# Creating text parser using tokenization\n",
    "parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
    "\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "# Summarize using sumy TextRank\n",
    "summarizer = TextRankSummarizer()\n",
    "summary_text =summarizer(parser.document,2)\n",
    "\n",
    "text_summary=\"\"\n",
    "for sentence in summary_text:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bf04f",
   "metadata": {},
   "source": [
    "## Lex Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60266487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the dawn of the internet, utilizing information has become pervasive but the rapid growth of information causes the problem of information overload.A recommender system computes and suggests the relevant items based on user details, content details, and their interaction logs such as ratings.\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "# Creating text parser using tokenization\n",
    "parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
    "\n",
    "\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "summarizer_lex = LexRankSummarizer()\n",
    "\n",
    "summary_lex =summarizer_lex(parser.document,2)\n",
    "\n",
    "text_summary=\"\"\n",
    "for sentence in summary_lex:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83401205",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655fa473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this context, Recommender System can help us to deal with such huge information.Content-based recommender system uses descriptive details products in order to make recommendations.\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "# Creating text parser using tokenization\n",
    "parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
    "\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "summarizer_lsa = LsaSummarizer()\n",
    "\n",
    "\n",
    "summary_lsa =summarizer_lsa(parser.document,2)\n",
    "\n",
    "text_summary=\"\"\n",
    "for sentence in summary_lsa:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45a780",
   "metadata": {},
   "source": [
    "## KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bb35866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A recommender system computes and suggests the relevant items based on user details, content details, and their interaction logs such as ratings.So the system adapts and learns the user behavior and suggests the items based on that behavior.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "\n",
    "summarizer_kl = KLSummarizer()\n",
    "\n",
    "# Summarize using sumy KL Divergence\n",
    "summary_kl =summarizer_kl(parser.document,2)\n",
    "\n",
    "text_summary=\"\"\n",
    "for sentence in summary_kl:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6061291",
   "metadata": {},
   "source": [
    "## Hybrid Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86c70bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary =  set(summary_kl).union(set(summary_lex)).union(set(summary_lsa)).union(set(summary_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4379d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A recommender system computes and suggests the relevant items based on user details, content details, and their interaction logs such as ratings.In this context, Recommender System can help us to deal with such huge information.So the system adapts and learns the user behavior and suggests the items based on that behavior.Content-based recommender system uses descriptive details products in order to make recommendations.With the dawn of the internet, utilizing information has become pervasive but the rapid growth of information causes the problem of information overload.For example, if the user has liked a web series in the past and the feature of that web series comedy genre then Recommender System will recommend the next series or movie based on the same genre.\n"
     ]
    }
   ],
   "source": [
    "text_summary=\"\"\n",
    "for sentence in summary:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "A recommender system computes and suggests the relevant items based on user details, content details, and their interaction logs such as ratings.In this context, Recommender System can help us to deal with such huge information.So the system adapts and learns the user behavior and suggests the items based on that behavior.\n",
    "\n",
    "\n",
    "x1,x2,x3 .. xn ==> y1, y2, y3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f718ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer generator ==> Extractive output\n",
    "T5 ==> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a39db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950e90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd0bbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175fa78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
